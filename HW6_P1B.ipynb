{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9h+ZT6ArhVLryiXAIW2aB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WestonMadeira1/HW_6/blob/main/HW6_P1B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uc9xihB0C9HL",
        "outputId": "2a71a326-8d95-40ce-efb2-ae6d9e93f8d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=12, out_features=36, bias=True)\n",
              "  (1): ReLU()\n",
              "  (2): Linear(in_features=36, out_features=64, bias=True)\n",
              "  (3): ReLU()\n",
              "  (4): Linear(in_features=64, out_features=16, bias=True)\n",
              "  (5): ReLU()\n",
              "  (6): Linear(in_features=16, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "file_path = '/content/drive/My Drive/ML Data/Housing.csv'\n",
        "housing_data = pd.read_csv(file_path)\n",
        "\n",
        "varlist =  ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea','furnishingstatus']\n",
        "\n",
        "def binary_map(x):\n",
        "    return x.map({'yes': 1, 'furnished':1, 'semi-furnished':0.5, 'unfurnished':0, 'no': 0})\n",
        "\n",
        "housing_data[varlist] = housing_data[varlist].apply(binary_map)\n",
        "housing_data.head()\n",
        "target_column = 'price'\n",
        "\n",
        "\n",
        "X = housing_data.drop(target_column, axis=1).values\n",
        "y = housing_data[target_column].values.reshape(-1, 1)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the input features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "# Convert numpy arrays to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32)\n",
        "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
        "\n",
        "# Create PyTorch DataLoader for training and validation sets\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "def training_loop(n_epochs, optimizer, model, loss_fn, t_u_train, t_u_val, t_c_train,\n",
        "                  t_c_val):\n",
        "  for epoch in range (1, n_epochs + 1) :\n",
        "    t_p_train = model (t_u_train)\n",
        "    loss_train = loss_fn(t_p_train, t_c_train)\n",
        "\n",
        "    t_p_val = model (t_u_val)\n",
        "\n",
        "    loss_val = loss_fn(t_p_val, t_c_val)\n",
        "    optimizer.zero_grad()\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch == 1 or epoch % 100 == 0:\n",
        "      print(f\"Epoch {epoch}, Training Loss {loss_train.item(): .4f}\",\n",
        "            f\"Validation Loss {loss_val.item():.4f}\")\n",
        "\n",
        "\n",
        "# Define the neural network model\n",
        "seq_model = nn.Sequential(\n",
        "            nn.Linear(12,36),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(36,64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64,16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16,1))\n",
        "seq_model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[param.shape for param in seq_model.parameters()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3T5ccBCvGKjM",
        "outputId": "3f16b595-ba0a-4916-86ec-031a21b66621"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[torch.Size([36, 12]),\n",
              " torch.Size([36]),\n",
              " torch.Size([64, 36]),\n",
              " torch.Size([64]),\n",
              " torch.Size([16, 64]),\n",
              " torch.Size([16]),\n",
              " torch.Size([1, 16]),\n",
              " torch.Size([1])]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in seq_model.named_parameters():\n",
        "  print(name, param.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvyp9lDTGkeY",
        "outputId": "caaf9677-603d-4c8c-ee7d-39924ad1aee4"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.weight torch.Size([36, 12])\n",
            "0.bias torch.Size([36])\n",
            "2.weight torch.Size([64, 36])\n",
            "2.bias torch.Size([64])\n",
            "4.weight torch.Size([16, 64])\n",
            "4.bias torch.Size([16])\n",
            "6.weight torch.Size([1, 16])\n",
            "6.bias torch.Size([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "optimizer = optim.Adam(seq_model.parameters(), lr=0.001)\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 500,\n",
        "    optimizer = optimizer,\n",
        "    model = seq_model,\n",
        "    loss_fn=nn.MSELoss(),\n",
        "    t_u_train = X_train_tensor,\n",
        "    t_u_val = X_val_tensor,\n",
        "    t_c_train = y_train_tensor,\n",
        "    t_c_val = y_val_tensor)\n",
        "\n",
        "#print('output', seq_model(X_val_tensor))\n",
        "#print('answer', y_val_tensor)\n",
        "#print('hidden', seq_model.0.weight.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oM2JkmwCG4P1",
        "outputId": "d09dd1e9-f307-470d-e95d-97595e329c89"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training Loss  25234788843520.0000 Validation Loss 30129992499200.0000\n",
            "Epoch 100, Training Loss  25234080006144.0000 Validation Loss 30129193484288.0000\n",
            "Epoch 200, Training Loss  25217176961024.0000 Validation Loss 30109610278912.0000\n",
            "Epoch 300, Training Loss  25118828920832.0000 Validation Loss 29994705223680.0000\n",
            "Epoch 400, Training Loss  24815765291008.0000 Validation Loss 29638887735296.0000\n",
            "Epoch 500, Training Loss  24145754587136.0000 Validation Loss 28849194663936.0000\n"
          ]
        }
      ]
    }
  ]
}