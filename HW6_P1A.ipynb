{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOnZuRHiNmA/prL6MdcAchA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WestonMadeira1/HW_6/blob/main/HW6_P1A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uc9xihB0C9HL",
        "outputId": "3a89eca3-7b9a-4df1-96d0-042c57cb37ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=12, out_features=36, bias=True)\n",
              "  (1): ReLU()\n",
              "  (2): Linear(in_features=36, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "file_path = '/content/drive/My Drive/ML Data/Housing.csv'\n",
        "housing_data = pd.read_csv(file_path)\n",
        "\n",
        "varlist =  ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea','furnishingstatus']\n",
        "\n",
        "def binary_map(x):\n",
        "    return x.map({'yes': 1, 'furnished':1, 'semi-furnished':0.5, 'unfurnished':0, 'no': 0})\n",
        "\n",
        "housing_data[varlist] = housing_data[varlist].apply(binary_map)\n",
        "housing_data.head()\n",
        "target_column = 'price'\n",
        "\n",
        "\n",
        "X = housing_data.drop(target_column, axis=1).values\n",
        "y = housing_data[target_column].values.reshape(-1, 1)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the input features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "# Convert numpy arrays to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32)\n",
        "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
        "\n",
        "# Create PyTorch DataLoader for training and validation sets\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "def training_loop(n_epochs, optimizer, model, loss_fn, t_u_train, t_u_val, t_c_train,\n",
        "                  t_c_val):\n",
        "  for epoch in range (1, n_epochs + 1) :\n",
        "    t_p_train = model (t_u_train)\n",
        "    loss_train = loss_fn(t_p_train, t_c_train)\n",
        "\n",
        "    t_p_val = model (t_u_val)\n",
        "\n",
        "    loss_val = loss_fn(t_p_val, t_c_val)\n",
        "    optimizer.zero_grad()\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch == 1 or epoch % 1000 == 0:\n",
        "      print(f\"Epoch {epoch}, Training Loss {loss_train.item(): .4f}\",\n",
        "            f\"Validation Loss {loss_val.item():.4f}\")\n",
        "\n",
        "\n",
        "# Define the neural network model\n",
        "seq_model = nn.Sequential(\n",
        "            nn.Linear(12,36),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(36,1))\n",
        "seq_model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[param.shape for param in seq_model.parameters()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3T5ccBCvGKjM",
        "outputId": "3b9dbd5f-1062-4d6a-f284-bdef8ffa19ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[torch.Size([36, 12]), torch.Size([36]), torch.Size([1, 36]), torch.Size([1])]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in seq_model.named_parameters():\n",
        "  print(name, param.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvyp9lDTGkeY",
        "outputId": "ec3ce4b5-8e38-40c3-d046-fd0d1d6d1812"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.weight torch.Size([36, 12])\n",
            "0.bias torch.Size([36])\n",
            "2.weight torch.Size([1, 36])\n",
            "2.bias torch.Size([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import Adam\n",
        "optimizer = optim.Adam(seq_model.parameters(), lr=0.001)\n",
        "\n",
        "training_loop(\n",
        "    n_epochs = 5000,\n",
        "    optimizer = optimizer,\n",
        "    model = seq_model,\n",
        "    loss_fn=nn.MSELoss(),\n",
        "    t_u_train = X_train_tensor,\n",
        "    t_u_val = X_val_tensor,\n",
        "    t_c_train = y_train_tensor,\n",
        "    t_c_val = y_val_tensor)\n",
        "\n",
        "#print('output', seq_model(X_val_tensor))\n",
        "#print('answer', y_val_tensor)\n",
        "#print('hidden', seq_model.0.weight.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oM2JkmwCG4P1",
        "outputId": "8f69739d-dbcd-42af-fa66-189ec0c1dd3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Training Loss  25234788843520.0000 Validation Loss 30129992499200.0000\n",
            "Epoch 1000, Training Loss  25232075128832.0000 Validation Loss 30126752399360.0000\n",
            "Epoch 2000, Training Loss  25224483438592.0000 Validation Loss 30117654953984.0000\n",
            "Epoch 3000, Training Loss  25213246898176.0000 Validation Loss 30104218501120.0000\n",
            "Epoch 4000, Training Loss  25198956904448.0000 Validation Loss 30087149780992.0000\n",
            "Epoch 5000, Training Loss  25181902864384.0000 Validation Loss 30066778046464.0000\n"
          ]
        }
      ]
    }
  ]
}